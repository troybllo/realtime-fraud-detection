{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11903a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.feature import StandardScaler, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"FraudDetectionFeatureEngineering\")\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n",
    "    .config(\"spark.driver.extraJavaOptions\", \"-Djava.security.manager=allow\")\n",
    "    .config(\"spark.executor.extraJavaOptions\", \"-Djava.security.manager=allow\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "df = spark.read.csv(\"../data/raw/creditcard.csv\", header=True, inferSchema=True)\n",
    "\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"user_id\", F.hash(F.concat(F.col(\"V1\"), F.col(\"V2\"), F.col(\"V3\"))) % 1000\n",
    ")\n",
    "\n",
    "df = df.withColumn(\"hour_of_day\", (F.col(\"Time\") % 86400) / 3600).withColumn(\n",
    "    \"day_of_week\", F.floor(F.col(\"Time\") / 86400) % 7\n",
    ")\n",
    "\n",
    "# 1. VELOCITY FEATURES - How fast are transactions happening?\n",
    "user_window = Window.partitionBy(\"user_id\").orderBy(\"Time\")\n",
    "time_window = Window.partitionBy(\"user_id\").orderBy(\"Time\").rowsBetween(-5, -1)\n",
    "\n",
    "df_features = df.withColumn(\n",
    "    \"prev_trans_time\", F.lag(\"Time\", 1).over(user_window)\n",
    ").withColumn(\n",
    "    \"time_since_last_trans\",\n",
    "    F.when(\n",
    "        F.col(\"prev_trans_time\").isNotNull(), F.col(\"Time\") - F.col(\"prev_trans_time\")\n",
    "    ).otherwise(999999),\n",
    ")\n",
    "\n",
    "# 2. ROLLING STATISTICS - User's recent behavior\n",
    "df_features = (\n",
    "    df_features.withColumn(\"rolling_mean_amount\", F.avg(\"Amount\").over(time_window))\n",
    "    .withColumn(\"rolling_std_amount\", F.stddev(\"Amount\").over(time_window))\n",
    "    .withColumn(\"rolling_count_trans\", F.count(\"*\").over(time_window))\n",
    ")\n",
    "\n",
    "# 3. ANOMALY SCORES - How unusual is this transaction?\n",
    "df_features = df_features.withColumn(\n",
    "    \"amount_zscore\",\n",
    "    F.when(\n",
    "        F.col(\"rolling_std_amount\") > 0,\n",
    "        (F.col(\"Amount\") - F.col(\"rolling_mean_amount\")) / F.col(\"rolling_std_amount\"),\n",
    "    ).otherwise(0),\n",
    ").withColumn(\n",
    "    \"unusual_hour\",\n",
    "    F.when((F.col(\"hour_of_day\") < 6) | (F.col(\"hour_of_day\") > 23), 1).otherwise(0),\n",
    ")\n",
    "\n",
    "# 4. TRANSACTION PATTERNS\n",
    "amount_window = Window.partitionBy(\"user_id\").orderBy(\"Time\").rowsBetween(-10, -1)\n",
    "df_features = (\n",
    "    df_features.withColumn(\n",
    "        \"trans_in_last_hour\",\n",
    "        F.sum(F.when(F.col(\"time_since_last_trans\") < 3600, 1).otherwise(0)).over(\n",
    "            time_window\n",
    "        ),\n",
    "    )\n",
    "    .withColumn(\"high_amount_flag\", F.when(F.col(\"Amount\") > 500, 1).otherwise(0))\n",
    "    .withColumn(\"very_low_amount_flag\", F.when(F.col(\"Amount\") < 1, 1).otherwise(0))\n",
    ")\n",
    "\n",
    "# 5. MERCHANT RISK FEATURES (simulated using V features)\n",
    "# In real world, you'd have actual merchant data\n",
    "df_features = df_features.withColumn(\n",
    "    \"merchant_risk_score\", F.abs(F.col(\"V1\") + F.col(\"V2\")) / 2\n",
    ").withColumn(\"location_risk_score\", F.abs(F.col(\"V3\") + F.col(\"V4\")) / 2)\n",
    "\n",
    "# 6. STATISTICAL OUTLIER DETECTION\n",
    "# Mahalanobis distance approximation using PCA components\n",
    "pca_cols = [f\"V{i}\" for i in range(1, 29)]\n",
    "df_features = df_features.withColumn(\n",
    "    \"pca_magnitude\", F.sqrt(sum([F.col(c) * F.col(c) for c in pca_cols]))\n",
    ")\n",
    "\n",
    "# 7. INTERACTION FEATURES\n",
    "df_features = df_features.withColumn(\n",
    "    \"amount_velocity_interaction\", F.col(\"Amount\") * F.col(\"trans_in_last_hour\")\n",
    ").withColumn(\"risk_time_interaction\", F.col(\"unusual_hour\") * F.col(\"high_amount_flag\"))\n",
    "\n",
    "# Show feature engineering results\n",
    "print(\"New features created:\")\n",
    "new_features = [\n",
    "    \"time_since_last_trans\",\n",
    "    \"rolling_mean_amount\",\n",
    "    \"rolling_std_amount\",\n",
    "    \"amount_zscore\",\n",
    "    \"unusual_hour\",\n",
    "    \"trans_in_last_hour\",\n",
    "    \"high_amount_flag\",\n",
    "    \"merchant_risk_score\",\n",
    "    \"pca_magnitude\",\n",
    "]\n",
    "df_features.select(\"Class\", *new_features).show(20)\n",
    "\n",
    "# Analyze feature importance for fraud detection\n",
    "fraud_feature_analysis = (\n",
    "    df_features.groupBy(\"Class\")\n",
    "    .agg(*[F.avg(col).alias(f\"avg_{col}\") for col in new_features])\n",
    "    .toPandas()\n",
    "    .T\n",
    ")\n",
    "\n",
    "print(\"Feature differences between fraud and normal:\")\n",
    "print(fraud_feature_analysis)\n",
    "\n",
    "# Save engineered features\n",
    "df_features.write.mode(\"overwrite\").parquet(\n",
    "    \"../data/processed/features_engineered.parquet\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
