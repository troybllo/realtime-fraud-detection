{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7812e508",
   "metadata": {
    "lines_to_next_cell": 3
   },
   "outputs": [],
   "source": [
    "# ---\n",
    "# jupyter:\n",
    "#   jupytext:\n",
    "#     formats: ipynb,py:percent\n",
    "#     text_representation:\n",
    "#       extension: .py\n",
    "#       format_name: percent\n",
    "#       format_version: '1.3'\n",
    "#       jupytext_version: 1.17.2\n",
    "#   kernelspec:\n",
    "#     display_name: Python 3\n",
    "#     language: python\n",
    "#     name: python3\n",
    "#\n",
    "#\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.classification import RandomForestClassifier, GBTClassifier, LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "import pyspark.sql.functions as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"FraudDetectionModeling\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load engineered features\n",
    "df = spark.read.parquet(\"../data/processed/features_engineered.parquet\")-\n",
    "\n",
    "# CLASS IMBALANCE\n",
    "\n",
    "fraud_count = df.filter(F.col(\"Class\") == 1).count()\n",
    "normal_count = df.filter(F.col(\"Class\") == 0).count()\n",
    "total_count = df.count()\n",
    "\n",
    "\n",
    "# calculates balances weights\n",
    "weight_fraud = total_count / (2 * fraud_count)\n",
    "weight_normal = total_count / (2 * normal_count)\n",
    "\n",
    "print(f\"Class weights - Fraud: {weight_fraud:.2f}, Normal : {weight_normal:.2f}\")\n",
    "\n",
    "# Add weight column\n",
    "df = df.withColumn(\"weight\",\n",
    "    F.when(F.col(\"Class\") == 1, weight_fraud).otherwise(weight_normal))\n",
    "\n",
    "#Balanced trainign sets with strategies\n",
    "\n",
    "#Strat A \n",
    "fraud_df = df.filter(F.col(\"Class\") == 1)\n",
    "normal_df = df.filter(F.col(\"Class\") == 0)\n",
    "balanced_df = normal_df.sample(fraction=fraud_count/normal_count, seed=42).union(fraud_df)\n",
    "\n",
    " # Strategy B: SMOTE (we'll implement a PySpark version)\n",
    " # Select feature columns\n",
    "feature_cols = [col for col in df.columns if col not in \n",
    "               ['Class', 'Time', 'user_id', 'prev_trans_time', 'weight']]\n",
    "\n",
    "# Create feature vector\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features_raw\")\n",
    "scaler = StandardScaler(inputCol=\"features_raw\", outputCol=\"features\")\n",
    "\n",
    "# Split data with stratification\n",
    "train_df, test_df = df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Ensure fraud cases in both sets\n",
    "print(f\"Train fraud rate: {train_df.filter(F.col('Class')==1).count() / train_df.count():.3%}\")\n",
    "print(f\"Test fraud rate: {test_df.filter(F.col('Class')==1).count() / test_df.count():.3%}\")\n",
    "\n",
    "#Model 1: Logistic Regression (Baseline)\n",
    "lr = LogisticRegression(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"Class\",\n",
    "    weightCol=\"weight\",\n",
    "    maxIter=100,\n",
    "    regParam=0.01,\n",
    "    elasticNetParam=0.5\n",
    ")\n",
    "\n",
    "#Model 2 Random Forest (Handles non-linearity)\n",
    "rf = RandomForestClassifier(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"Class\",\n",
    "    weightCol=\"weight\",\n",
    "    numTrees=100,\n",
    "    maxDepth=10,\n",
    "    featureSubsetStrategy=\"sqrt\",\n",
    "    subsamplingRate=0.8\n",
    ")\n",
    "\n",
    "#Model 3: Gradient Boosted Trees (Best of imbalanced data)\n",
    "gbt = GBTClassifier(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"Class\",\n",
    "    weightCol=\"weight\",\n",
    "    maxIter=50,\n",
    "    maxDepth=5,\n",
    "    stepSize=0.1,\n",
    "    subsamplingRate=0.8\n",
    ")\n",
    "\n",
    "# Create pipelines \n",
    "models = {\n",
    "    \"Logistic Regression\": Pipeline(stages=[assembler,scaler,lr]),\n",
    "    \"Random Forest\": Pipeline(stages=[assembler,scaler,rf]),\n",
    "    \"Gradient Boosted Trees\": Pipeline(stages=[assembler,scaler,gbt])\n",
    "}\n",
    "\n",
    "# Custom evaluation fucntion\n",
    "def evaluate_fraud_model(model,test_df,model_name):\n",
    "    predictions = model.transform(test_df)\n",
    "    pred_pd = predictions.select(\"Class\", \"prediction\", \"probability\").toPandas()\n",
    "    pred_pd['fraud_probability'] = pred_pd['probability'].apply(lambda x: x[1])\n",
    "\n",
    "    # 1. Base Metrics \n",
    "    from sklearn.metrics import classification_report, roc_auc_score\n",
    "    print(f\"\\n{model_name} Results:\")\n",
    "    print(classification_report(pred_pd['Class'], pred_pd['prediction'], target_names=['Normal','Fraud']))\n",
    "\n",
    "    # 2. ROC-AUC (Critical for imbalanced data)\n",
    "    roc_auc = roc_auc_score(pred_pd['Class'],pred_pd['fraud_probability'])\n",
    "    print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n",
    "\n",
    "    # 3. Precision-Recall Curve (More information than ROC for imbalanced)\n",
    "    from sklearn.metrics import average_precision_score\n",
    "    avg_precision = average_precision_score(pred_pd['Class'], pred_pd['fraud_probability'])\n",
    "    print(f\"Average Precision Score: {avg_precision:.4f}\")\n",
    "    \n",
    "    # 4. Cost-Based Evaluation (Business Impact)\n",
    "    # Assume: False Negative costs $100, False Positive costs $1\n",
    "    cm = confusion_matrix(pred_pd['Class'], pred_pd['prediction'])\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    \n",
    "    cost = (fn * 100) + (fp * 1)\n",
    "    print(f\"Total Cost: ${cost:,}\")\n",
    "    print(f\"Fraud Caught: {tp}/{tp+fn} ({tp/(tp+fn):.1%})\")\n",
    "    \n",
    "    # 5. Visualization\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    sns.heatmap(cm, annot=True, fmt='d', ax=axes[0,0])\n",
    "    axes[0,0].set_title(f'Confusion Matrix - {model_name}')\n",
    "    axes[0,0].set_xlabel('Predicted')\n",
    "    axes[0,0].set_ylabel('Actual')\n",
    "    \n",
    "    # ROC Curve\n",
    "    fpr, tpr, _ = roc_curve(pred_pd['Class'], pred_pd['fraud_probability'])\n",
    "    axes[0,1].plot(fpr, tpr, label=f'ROC (AUC = {roc_auc:.3f})')\n",
    "    axes[0,1].plot([0, 1], [0, 1], 'k--')\n",
    "    axes[0,1].set_xlabel('False Positive Rate')\n",
    "    axes[0,1].set_ylabel('True Positive Rate')\n",
    "    axes[0,1].set_title('ROC Curve')\n",
    "    axes[0,1].legend()\n",
    "    \n",
    "    # Precision-Recall Curve\n",
    "    precision, recall, _ = precision_recall_curve(pred_pd['Class'], pred_pd['fraud_probability'])\n",
    "    axes[1,0].plot(recall, precision, label=f'AP = {avg_precision:.3f}')\n",
    "    axes[1,0].set_xlabel('Recall')\n",
    "    axes[1,0].set_ylabel('Precision')\n",
    "    axes[1,0].set_title('Precision-Recall Curve')\n",
    "    axes[1,0].legend()\n",
    "    \n",
    "    # Probability Distribution\n",
    "    axes[1,1].hist(pred_pd[pred_pd['Class']==0]['fraud_probability'], \n",
    "                   bins=50, alpha=0.5, label='Normal', density=True)\n",
    "    axes[1,1].hist(pred_pd[pred_pd['Class']==1]['fraud_probability'], \n",
    "                   bins=50, alpha=0.5, label='Fraud', density=True)\n",
    "    axes[1,1].set_xlabel('Fraud Probability')\n",
    "    axes[1,1].set_ylabel('Density')\n",
    "    axes[1,1].set_title('Probability Distribution by Class')\n",
    "    axes[1,1].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return predictions, roc_auc\n",
    "\n",
    "# Train and evaluate all models\n",
    "results = {}\n",
    "for name, pipeline in models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    model = pipeline.fit(train_df)\n",
    "    predictions, auc = evaluate_fraud_model(model, test_df, name)\n",
    "    results[name] = {'model': model, 'auc': auc, 'predictions': predictions}\n",
    "\n",
    "def optimize_threshold(predictions_df, cost_fn=100, cost_fp=1):\n",
    "    \"\"\"Find optimal threshold balancing fraud detection and false positives\"\"\"\n",
    "    \n",
    "    pred_pd = predictions_df.select(\"Class\", \"probability\").toPandas()\n",
    "    pred_pd['fraud_prob'] = pred_pd['probability'].apply(lambda x: x[1])\n",
    "    \n",
    "    thresholds = np.linspace(0, 1, 100)\n",
    "    costs = []\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        pred_pd['pred'] = (pred_pd['fraud_prob'] >= threshold).astype(int)\n",
    "        cm = confusion_matrix(pred_pd['Class'], pred_pd['pred'])\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "        cost = (fn * cost_fn) + (fp * cost_fp)\n",
    "        costs.append(cost)\n",
    "    \n",
    "    optimal_idx = np.argmin(costs)\n",
    "    optimal_threshold = thresholds[optimal_idx]\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(thresholds, costs)\n",
    "    plt.axvline(optimal_threshold, color='r', linestyle='--', \n",
    "                label=f'Optimal Threshold: {optimal_threshold:.3f}')\n",
    "    plt.xlabel('Threshold')\n",
    "    plt.ylabel('Total Cost ($)')\n",
    "    plt.title('Cost vs Threshold')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    return optimal_threshold\n",
    "\n",
    "# Find optimal threshold for best model\n",
    "best_model_name = max(results, key=lambda x: results[x]['auc'])\n",
    "optimal_thresh = optimize_threshold(results[best_model_name]['predictions'])"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
